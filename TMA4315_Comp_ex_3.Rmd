--- 
title: "TMA4315: Compulsory exercise 3: (Generalized) Linear Mixed Models" 
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group XX: Henrik Syversveen Lie, Mikal Stapnes, Oliver Byhring'
---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
library(ggplot2)
```

# a)
```{r, echo = F, eval = T}
library(GGally)
dataset <- read.table("https://www.math.ntnu.no/emner/TMA4315/2018h/jsp2.txt", header = TRUE)
indices = dataset$gender == 'boy'

mean(dataset[dataset$gender == 'boy', ]$math)
mean(dataset[dataset$gender == 'girl', ]$math)
ggpairs(data = dataset, mapping = aes(col = gender), columns = c("social", "raven", "math"), legend = 1)
```

* Comment briefly on the plot you have created

First, we see that there is a positive correlation between the `raven` (test score) and `math` variable. This is as expected. Furthermore, we see that girls perform somewhat better in the math test than boys. Also, there is no evident correlation between social class and test scores, which is somewhat surprising.


```{r, echo = F, eval = T}
fit = lm(math~raven+gender, data = dataset)
summary(fit)
```

We fit a linear model with `math` as response, and `raven` and `gender` as covariates. Model for the $k$th student:

$$Y_k=\mathbf{x}_k\boldsymbol{\beta}+\epsilon_k$$

* Explain what the different parts of this model are called.

$Y_k$ is called the response or random component, and will be the `math` score of student $k$. We assume the response to be normal distributed. The term $\mathbf{x}_k\boldsymbol{\beta}$ is the systematic component, or linear predictor. The final term $\epsilon_k$ is called the random error component, and is normal distributed with mean 0 and variance $\sigma^2$.

* Comment briefly on the parameter estimates you have found.

All parameter estimates are significant on a $0.001$ level. We observe that the coefficient $\beta_0 = -1.3131$, $\beta_{raven} = 0.1965$ and $\beta_{girl} = 2.5381$, which means that if $x_{raven} \rightarrow x_{raven} + 1$ our model would predict an increase in the math score of $0.1965$. Similarly for `girl`, our model will predict a `math` score that is $2.5381$ higher for a girl than for a boy assuming the remaining covariates are equal. Finally, from the intercept coefficient we see that a boy with 0 raven score is expected to get -1.3131 on the math test.

* What are we investigating with this model?

With this model we assume a linear relationship between the respone $Y =$ `math` and the covariates `raven` and `gender` and a normal distribution of the residuals,

$$ Y_k = x_k^T \beta + \epsilon_k, \quad \epsilon_k \sim N(0, \sigma^2) $$

Under these assumptions we investigate the significance and strength of our parameters $\beta_{raven}$ and $\beta_{girl}$. We found both that the parameters are significant and that they are relatively strong. 

# b)

However, this model assumes that the distribution of `raven` and `gender` is independent of other covariates, which is not necessarily true. If there has been some gender distribution among the good and bad schools, this `school` effect will affect the parameter estimation $\beta_{gender}$ and we will not be able to distinguish what should be attributed to the gender and what to the schools. 

We therefore want to include in our model a random intercept $\gamma_{0, school}$ that seeks to remove the `school` factor as a contributing effect of the other parameters estimates. 

We fit a new model
$$\mathbf{Y}_i = \mathbf{X}_i\beta + \mathbf{1}\gamma_{0i} + \epsilon_i.$$


* Explain what the different parts of this model are called and what dimensions the model components have.

The vector $\mathbf{Y}_i$ is still the response (math score) or random component for all students from school $i$ and has dimension $n_i\times1$. The term systematic component or linear predictor is now $\mathbf{X}_i\beta+\mathbf{1}\gamma_{0i}$. $\mathbf{X}_i$ is a $n_i\times p$ design matrix, $\beta$ is a $p\times1$ vector of fixed coefficients and $\mathbf{1}$ is a $n_i\times1$ vector (design matrix for random effects) with every element equal 1. $\gamma_{0i}$ is the random intercept, which is a normal distributed scalar with mean 0 and variance $\tau_0^2$. Finally, the term $\epsilon_i$ is the random error component, which is a $n_i\times1$ normal distributed vector with mean 0 and variance $I\sigma^2$.


* Write down distributional assumptions for $\gamma_{0i}$ and $\epsilon_i$.

We assume that each $\epsilon_i$ is independent, and that each of its elements are independent. This gives $\epsilon_i \sim N_{n_i}(0,I\sigma^2)$. Also, we assume $\gamma_{0i}$ i.i.d. with $\gamma_{0i} \sim N_1(0,\tau_0^2)$. 

* What do we assume about the dependency between the responses at school $i$, $\mathbf{Y}_i$, and school $k$, $\mathbf{Y}_k$?

We assume that responses between different schools $i$ and $k$ are conditionally independent. This means that if we construct a new global model for all clusters, with
$$\mathbf{Y}= \left(\begin{array}{c} 
\mathbf{Y}_1 \\ 
\mathbf{Y}_2 \\
\vdots \\
\mathbf{Y}_m
\end{array}\right),$$

then each $\mathbf{Y}_{ij}$ will be conditionally independent, or $\mathbf{Y}|\boldsymbol{\gamma} \sim N(\mathbf{X}\boldsymbol{\beta} + \mathbf{U}\boldsymbol{\gamma},\boldsymbol{I}\sigma^2)$.

Now we fit the model in `R`:
```{r, echo = F, eval = T}
library(lme4)
fitRI1 <- lmer(math ~ raven + gender + (1 | school), data = dataset)
summary(fitRI1)
```

* Compare the parameter estimates for `raven` and `gender` with the estimates from the linear model in a), and discuss.

We see that the parameters `gender` and `raven` (and the intercept) are approximately the same for the two models. Because we have fairly balanced data (almost same amount of boy/girl students in every school), the parameters will be almost the same when all students have the same intercept vs. when every school has its own intercept.

* How do `gender` and `raven` score affect the math scores?

Again, we can say that for one female and one male student from the same school with equal `raven` score, the female student is expected to get 2.51119 better score on the math test. Also, if a student increases his/her `raven` score by one, we would expect an increase of 0.21442 to the math score.

* In the print-out from `summary(fitRI1)` there are no p-values. Why is this?

The distribution of the $\beta$s is assymptotically normal, but for finite sample sizes, we are not sure if the usual t-statistic is t-distributed. Also, if it is t-distributed, we do not know the degrees of freedom.

* Test the null-hypothesis $H_0:\ \beta_{\text{raven}}=0$ against $H_1:\ \beta_{\text{raven}}\neq0$ and provide a p-value for the test. (Yes, we have many observations and believe that we can calculate an asymptotic p-value even though the lmer package not by default want to report such a number.)

We assume we have enough observations to calculate an asymptotic p-value. We compute
```{r, echo = T, eval = T}
2*(1-pnorm(9.197))
```
We observe that we get an asymptotic p-value of 0 for testing the hypothesis $H_0:\ \beta_{\text{raven}}=0$ against $H_1:\ \beta_{\text{raven}}\neq0$. We reject $H_0$ and conclude that $\beta_{\text{raven}}\neq0$.


* Also provide a $95\%$ confidence interval for the effect of the female gender on the math score.

We assume that we have enough observations for the $\beta$'s to be normally distributed. We can then compute a $95\%$ confidence interval for the effect of female gender to be
$$\beta_{\text{girl}} \in [\hat{\beta}_{\text{girl}} - z_{0.025}\text{sd}(\beta_{\text{girl}}), \hat{\beta}_{\text{girl}} + z_{0.025}\text{sd}(\beta_{\text{girl}})].$$
And this gives
$$\beta_{\text{girl}} \in [2.51119 - 1.96\cdot0.26684, 2.51119 + 1.96\cdot0.26684].$$
$$\beta_{\text{girl}} \in [1.988, 3.034].$$


# c)

We now continue with a random intercept model (school) with only raven as fixed effect (remove gender from our model).

```{r, echo = F, eval = T}
fitRI2 <- lmer(math ~ raven + (1 | school), data = dataset, reml=T)
summary(fitRI2)
```

We rewrite our model as
$$\mathbf{Y}_i = \mathbf{X}_i\boldsymbol{\beta} + \mathbf{1}\gamma_{0i} + \epsilon_i = \mathbf{X}_i\boldsymbol{\beta} + \epsilon_i^*.$$
We then write $\mathbf{V}_i = \text{Cov}(\epsilon_i^*) = \text{Cov}(\mathbf{1}\gamma_{0i}) + \text{Cov}(\epsilon_i) = \mathbf{1}\tau_0^2\mathbf{1}^T + \mathbf{I}\sigma^2$. This means that the matrix $\mathbf{V}_i$ will be on the form

$$\mathbf{V}_i = \left(\begin{array}{cccc} 
\tau_0^2 + \sigma^2 & \tau_0^2 & \dots & \tau_0^2 \\ 
\tau_0^2 & \tau_0^2 + \sigma^2 & \dots & \tau_0^2 \\
\vdots & \vdots & \ddots & \vdots\\
\tau_0^2 & \tau_0^2  & \dots & \tau_0^2+ \sigma^2 
\end{array}\right),$$
an $n_i\times n_i$ matrix with $tau_0^2$ on the off-diagonal and $\tau_0^2+\sigma^2$ on the diagonal. This gives

$$\mathbf{Y}_i \sim N(\mathbf{X}_i\boldsymbol{\beta}, \mathbf{V}_i).$$
In conclusion, we can say that the covariance between the responses $Y_{ij}$ and $Y_{il}$ from school $i$ is $\tau_0^2$, and the correlation is $\tau_0^2/(\tau_0^2+\sigma^2)$.

* What is this correlation for our fitted model `fitRI2`? Comment.

The `R` printout gives $\sigma^2 = 20.711$ and $\tau_0^2 = 4.002$, which gives a correlation of $4.002/(20.711+4.002) = 0.1619$. We observe that the correlation always has to be positive, because we only have square terms in the formula for the correlation.


* Write down the mathematical formula for $\hat{\gamma}_{0i}$ for your random intercept model and explain what the different elements in the formula means.

According to the module pages, it can be shown that after some calculations we get
$$\hat{\gamma}_{0i} = \hat{\mathbf{Q}}\mathbf{U}_i^T\hat{\mathbf{V}}_i^{-1}(\mathbf{Y}_i-\mathbf{X}_i\boldsymbol{\beta})=\dots=\frac{n_i\hat{\tau}_0^2}{\hat{\sigma}^2+n_i\hat{\tau}_0^2}e_i.$$

Here, $n_i$ is the number of observations in cluster $i$. $\hat{\tau}_0^2$ is the estimated variance of each $\gamma_{0i}$. $\hat{\sigma}^2$ is the estimated variance of $\epsilon_i$. Finally, $e_i$ is the average (raw, level 0) residual given by 
$$e_i = \frac{1}{n_i}\sum_{j=1}^{n_i}(Y_{ij}-\mathbf{x}_{ij}^T\hat{\boldsymbol{\beta}}).$$

* Explain what each of the six plots produced and displayed below can be used for (that is, why are we asking you to make these plots).

##QQ-plot of random intercept:
Plots random intercept quantiles on the y-axis vs. standard normal quantiles on the x-axis. Can be used to check normality assumption of the random intercept.

##Random intercept(RI)
Essentially the same plot as the last one, with random intercept quantiles on the x-axis and schools on the y-axis. Displays all the random intercepts quantiles, and can be used to check normality assumption of random intercept.

##Density of RI
Plots the density function of the random intercept vs. a theoretical normal distribution to check if the distribution of the random intercept replicates a normal distribution.

##Residuals vs Fitted values
Plots the residuals vs. fitted values of the regression to check if the residuals are i.i.d. normal.

##Normal Q-Q
Plots standardized residuals of the regression vs. theoretical quantiles. Can be used to check normality assumption of the response $Y$.

##Last plot
Plots the math score as a function of raven score for all the different schools. Can be used for prediction of a math score for a given school and given raven score.

* Comment on your findings.

```{r, echo = F, eval = T}
library(ggplot2)
library(sjPlot)
library(ggpubr)
gg1 <- plot_model(fitRI2, type = "diag", prnt.plot = FALSE, geom.size = 1)
gg2 <- plot_model(fitRI2, type = "re", sort.est = "(Intercept)", y.offset = 0.4, dot.size = 1.5) +
theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
labs(title = "Random intercept (RI)", x = "school", y = "math")
gg3 <- ggplot(data = data.frame(x = ranef(fitRI2)$school[[1]]), aes(x = x)) + geom_density() +
labs(x = "math", y = "density", title = "Density of RI") +
stat_function(fun = dnorm, args = list(mean = 0, sd = attr(VarCorr(fitRI2)$school, "stddev")), col = "red")
df <- data.frame(fitted = fitted(fitRI2), resid = residuals(fitRI2, scaled = TRUE))
gg4 <- ggplot(df, aes(fitted,resid)) + geom_point(pch = 21) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
labs(x = "Fitted values", y = "Residuals", title = "Residuals vs Fitted values")
gg5 <- ggplot(df, aes(sample=resid)) + stat_qq(pch = 19) +
geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q")
gg1[[2]]$school + ggtitle("QQ-plot of random intercepts")
ggarrange(gg2, gg3, gg4, gg5)
df <- data.frame(x = rep(range(dataset$raven), each = 49),
                 y = coef(fitRI2)$school[,1] + coef(fitRI2)$school[,2] * rep(range(dataset$raven), each = 49),
                 School = factor(rep(c(1:42, 44:50), times = 2)))
ggplot(df, aes(x = x, y = y, col = School)) + geom_line() + labs(x = "raven score", y = "math score")
```

From the first plot it seems like the random intercept is indeed normal, with only the quantile of one school really deviating from the normal line. From the density plot, we see that the distribution function is somewhat "thinner" than a normal distribution, with smaller tails and higher density for small absolute values of the random intercept. Moreover, the residuals seem to be homoscedastic and uncorrelated. The normal Q-Q plot also supports the assumption of normality of the residuals. From the last plot we see that one school has significant lower math scores than the rest, but apart from that there are no evident violations of our model assumptions.

# d)

We now compare the random intercept models with and without the social status of the father.

```{r, echo = F, eval = T}
fitRI3 <- lmer(math ~ raven + social + (1 | school), data = dataset, REML=T)

anova(fitRI2, fitRI3)
cat("P(chi^2 > 8.1175) = ", 1 - pchisq(8.1175, 3))
```

From the `anova` printout we get a reduction in `deviance` of $8.1175$ when with $3$ additional parameters. Under model assumptions, the reduction in `deviance` will be $\chi^2$-distributed with $3$ degrees of freedom, which gives a p-value of $0.0436$. As this is significant on a $0.05$-level, we would prefer the larger model that includes the social status as a covariate. 

We know that the REML estimation for the parameters $(\sigma^2, \tau_0^2)$ in $V = \sigma^2I + UGU^T$ are found by maximizing the likelihood for $A^T Y$, where $A$ is any $N \times (N-p)$ full-rank matrix with columns orthogonal to the columns of the design matrix $X$. However, as $p$ differs between the two models, the transformations $A_2$ and $A_3$ will differ and the transformed observations $A_2^T Y$ and $A_3^T T$ will not be the same. As the observations differ, the likelihood of the respective saturated models will also differ and we cannot use the Likelihood Ratio Test. Instead we must use the Maximum-Likelihood (ML) estimation to conduct the Likelihood Ratio Test and use Restriced Maximum Likelihood (REML) for parameter estimation. 

Both AIC and BIC add a small penalty to the `deviance` for the use of additional parameters in the model. We would prefer the model with the least AIC/BIC, but we see here that the two measures do not agree on which model is better. However, as we here have nested models and can use the Likelihood Ratio Test, we prefer this over either measure and can disregard both the AIC and the BIC. 

The last model we want to consider is a model with a random intercept and a random slope for the `raven` score at each school. 

```{r}
fitRIS <- lmer(math ~ raven + (1 + raven | school), data = dataset)
summary(fitRIS)
```

```{r}
df <- data.frame(x = rep(range(dataset$raven), each = 49),
y = coef(fitRIS)$school[,1] + coef(fitRIS)$school[,2] * rep(range(dataset$raven), each = 49),
School = factor(rep(c(1:42, 44:50), times = 2)))
gg1 <- ggplot(df, aes(x = x, y = y, col = School)) + geom_line()
gg2 <- plot_model(fitRIS, type = "re", sort.est = "(Intercept)", y.offset = 0.4, dot.size = 1.5) +
theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) + labs(title = "Random intercept (RI)")
ggarrange(gg1, gg2, ncol = 2, legend = FALSE)
```

We assume the model

$$ Y_{i} = \beta_0 + \beta_1 raven + \gamma_{i0} + \gamma_{i1} raven + \epsilon_{i},\\
\epsilon_i \sim N_{n_i} (\mathbf{0}, \sigma^2 I_{n_i}), \\
\gamma_{i} = (\gamma_{i0}, \gamma_{i1}) \sim N_2 (\mathbf{0}, Q)$$

We now permit both a random effect for `raven`, $\gamma_{i1}$ and a random intercept $\gamma_{i0}$ for each school $i$. Observing the spread in the fitted lines $\hat{Y}_i(raven)$ we could support our claim that the inclusion of a random effect $\gamma_{i1}$ is reasonable.

However, from the right plot we observe that the random effects tend to cancel each other out; where the random intercept is positive the random effects are mostly negative and vice versa. We observe from the summary of the model that the effects are negatively correlated, $Corr(\gamma_{i0}, \gamma_{i1}) = -0.40$. It is possible that the random effects "blow each other up", and the inclusion of the random effect $\gamma_{i1}$ may result in an overestimate of the fixed effect $\beta_{raven}$ ($0.2498$ vs $0.2068$).

# e)

* Why is it not suitable to use a linear mixed effects model?

We now wish our model to predict a probability in the range $[0, 1]$. It is no longer suitable to use a linear mixed effects model because our random effects are bounded and assuming normality of them is no longer reasonable.

* What type of model would be more suitable? (hint: IL module 8)

To create a reasonable model we must instead consider som transformation$t(\cdot): [0, 1] \rightarrow (-\infty, \infty)$. In this case, it will be suitable to consider a generalized linear mixed model with a binomial response. We then assume

$$\eta_{ij} \sim x_{ij}^T \beta + u_{ij}^T \gamma_i + \epsilon_{ij} \\
\gamma_i \sim N(\mathbf{0}, Q) \\
\epsilon_{ij} \sim N(0, \sigma^2)$$ 

and use the transformation

$$Y_{ij} = \frac{\exp{\eta_{ij}}}{1+\exp{\eta_{ij}}},$$

as this is the canonical link of the binomial distribution. 

* How would we add a random school intercept into this model (in which part of the model)?

To add a random school intercept into this model we simply let $\gamma_i = \gamma_{i0}$ denote each random school intercept and assume

$$\eta_{ij} \sim \beta_0 + \beta_1 \text{raven} + \gamma_i + \epsilon_{ij}, \\
\gamma_i \sim N(0, \tau_0^2), \\
\epsilon_{ij} \sim N(0, \sigma^2)$$ 

* What is the main challenge with this type of models? (hint: marginal model)

To do parameter estimation, we the consider the likelihood

$$L(\mathbf{\theta}) = \prod_{i=1}^m f(y_{ij} \lvert \mathbf{\theta}) = \prod_{i=1}^m \int_{\gamma_i} f(y_{ij} \lvert \gamma_i, \mathbf{\theta})f(\gamma_i)d\gamma_i,$$

and observe that, unfortunately, the expression $f(y_{ij} \lvert \mathbf{\theta}) = \int_{\gamma_i} f(y_{ij} \lvert \gamma_i, \mathbf{\theta})f(\gamma_i)d\gamma_i$ only has a known distribution in special cases. Thus, to calculate the likelihood (and log-likelihood, deviance, BIC, AIC and other measures dependent on the likelihood) of a GLMM we must approximate this integral numerically.